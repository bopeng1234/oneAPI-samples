{
	"guid": "01341A25-5FBF-4AE5-B4C9-B84105C8D3E9",
	"name": "Quantize Transformer Models using IntelÂ® Extension for Transformers* (ITREX)",
	"categories": ["Toolkit/oneAPI AI And Analytics/Features And Functionality"],
	"description": "Quantizing Transformer models in a step-by-step manner to enable memory efficient LLM inference.",
	"builder": ["cli"],
	"languages": [{
		"python": {}
	}],
	"os": ["linux"],
	"targetDevice": ["CPU"],
	"ciTests": {
		"linux": [{
			"env": [
				"source /intel/oneapi/intelpython/bin/activate",
				"conda activate pytorch",
				"pip install uv",
				"uv init",
				"uv python pin $(which python)",
				"uv venv --system-site-packages",
				"uv add -r requirements.txt",
				"uv add neural-compressor==2.6",
				"uv add torch==2.5.0 torchvision==0.20.0"
            ],
			"id": "itrex_quantize_transformer_models",
			"steps": [
				"uv run python quantize_transformer_models_with_itrex.py --model_name Intel/neural-chat-7b-v3-1 --quantize int4 --max_new_tokens 50"
			]
		}]
	},
    "expertise": "Concepts and Functionality"
}
