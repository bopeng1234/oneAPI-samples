{
    "guid": "0A887217-5621-4C8D-9418-17558088698B",
    "name": "IntelÂ® Extension for TensorFlow* BF16 Inference",
    "categories": ["Toolkit/oneAPI AI And Analytics/Features and Functionality"],
    "description": "This sample illustrates how to inference a TensorFlow model using Advanced Matrix Extensions Bfloat16",
    "builder": ["cli"],
    "languages": [{"python":{}}],
    "os":["linux"],
    "targetDevice": ["CPU"],
    "cpuInstructionSets": ["AVX512", "AMX"],
    "ciTests": {
      "linux": [
      {
          "id": "intel amx bf16 inference",
          "steps": [
              "source /intel/oneapi/intelpython/bin/activate",
              "conda activate tensorflow",
              "pip install uv",
              "uv init",
              "uv python pin $(which python)",
              "uv venv --system-site-packages",
              "uv add -r requirements.txt",
              "uv add numpy==1.26.4",
              "uv add ipykernel jupyter notebook",
              "uv run python Intel_TensorFlow_AMX_BF16_Inference.py",
              "uv run python -m ipykernel install --user --name=tensorflow",
              "uv run jupyter nbconvert --ExecutePreprocessor.enabled=True --ExecutePreprocessor.kernel_name=tensorflow2 --to notebook IntelTensorFlow_AMX_BF16_Inference.ipynb"
           ]
      }
       ]
    },
    "expertise": "Code Optimization"
  }
  
